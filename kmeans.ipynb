{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "77be8482958b167d0ba9dae5c81c4a788ebb3117c9dd1247886c2e356be97fa6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "maximos y minimos 1139.5931448542399 -84.87695888065039\n",
      "0%\n",
      "1%\n",
      "2%\n",
      "3%\n",
      "4%\n",
      "5%\n",
      "6%\n",
      "7%\n",
      "8%\n",
      "9%\n",
      "10%\n",
      "11%\n",
      "12%\n",
      "13%\n",
      "14%\n",
      "15%\n",
      "16%\n",
      "17%\n",
      "18%\n",
      "19%\n",
      "20%\n",
      "21%\n",
      "22%\n",
      "23%\n",
      "24%\n",
      "25%\n",
      "26%\n",
      "27%\n",
      "28%\n",
      "29%\n",
      "30%\n",
      "31%\n",
      "32%\n",
      "33%\n",
      "34%\n",
      "35%\n",
      "36%\n",
      "37%\n",
      "38%\n",
      "39%\n",
      "40%\n",
      "41%\n",
      "42%\n",
      "43%\n",
      "44%\n",
      "45%\n",
      "46%\n",
      "47%\n",
      "48%\n",
      "49%\n",
      "50%\n",
      "51%\n",
      "52%\n",
      "53%\n",
      "54%\n",
      "55%\n",
      "56%\n",
      "57%\n",
      "58%\n",
      "59%\n",
      "60%\n",
      "61%\n",
      "62%\n",
      "63%\n",
      "64%\n",
      "65%\n",
      "66%\n",
      "67%\n",
      "68%\n",
      "69%\n",
      "70%\n",
      "71%\n",
      "72%\n",
      "73%\n",
      "74%\n",
      "75%\n",
      "76%\n",
      "77%\n",
      "78%\n",
      "79%\n",
      "80%\n",
      "81%\n",
      "82%\n",
      "83%\n",
      "84%\n",
      "85%\n",
      "86%\n",
      "87%\n",
      "88%\n",
      "89%\n",
      "90%\n",
      "91%\n",
      "92%\n",
      "93%\n",
      "94%\n",
      "95%\n",
      "96%\n",
      "97%\n",
      "98%\n",
      "100%\n",
      "Cluster: 0 \n",
      "\n",
      "['liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver']\n",
      "Cluster: 1 \n",
      "\n",
      "['cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "Cluster: 2 \n",
      "\n",
      "['hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus']\n",
      "Cluster: 3 \n",
      "\n",
      "['cerebellum', 'cerebellum', 'liver', 'liver', 'kidney', 'kidney', 'placenta', 'placenta', 'placenta']\n",
      "Cluster: 4 \n",
      "\n",
      "['cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "Cluster: 5 \n",
      "\n",
      "['kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'placenta', 'placenta', 'placenta']\n",
      "Cluster: 6 \n",
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import random\n",
    "#ORDEN DE REVISIÓN SUGERIDO: \n",
    "# 1) kmeans()\n",
    "# 2) load()\n",
    "# 3) iteration()\n",
    "# 4) distance()\n",
    "#EL MAIN SE ENCUENTRA AL PIE DEL ARCHIVO\n",
    "\n",
    "\n",
    "def distance(a, b):\n",
    "    #DISTANCIA EUCLIDIANA N DIMENSIONAL\n",
    "    aux = np.sum(np.square(a - b))\n",
    "    return np.sqrt(aux)\n",
    "\n",
    "\n",
    "def iteration(k_puntos, puntos, dim_datos):\n",
    "    #CALCULO DE DISTANCIA DE CADA DATO CON LOS K CENTROIDES DE LOS CLUSTERES Y ASIGNAMIENTO DEL CLUSTER MAS CERCANO PARA CADA DATO\n",
    "    clusters = []\n",
    "    for idx, punto in enumerate(puntos):\n",
    "        d = 99999999999991\n",
    "        ki = 0\n",
    "        for idx_k, k_i in enumerate(k_puntos):\n",
    "            if(d > distance(punto, k_i)):\n",
    "                d = distance(punto, k_i)\n",
    "                ki = idx_k\n",
    "        clusters.append(ki)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #CONTABILIZACION DE ELEMENTOS POR CADA CLUSTER Y SUMA DE FEATURES PARA CADA UNO DE ELLOS\n",
    "    aux = [0] * dim_datos\n",
    "    means = [np.array(aux)] * len(puntos)\n",
    "    count = [0] * len(puntos)\n",
    "    for i in range(0,len(puntos)):\n",
    "        idx = clusters[i]\n",
    "        means[idx] = np.add(puntos[i], means[idx])\n",
    "        count[idx] += 1\n",
    "\n",
    "\n",
    "    #CÁLCULO DE LA MEDIA PARA CADA CLUSTER Y REASIGNACIÓN DE LOS K CENTROIDES\n",
    "    k_p = []\n",
    "    for j in range(len(means)):\n",
    "        # print(means[i])\n",
    "        # print(count[i])\n",
    "        # break\n",
    "        if(count[j] != 0):\n",
    "            #print('a')\n",
    "            for i in range(len(means[j])):\n",
    "                means[j][i] = means[j][i]/count[j]\n",
    "            k_p.append(np.array(means[j]))\n",
    "    k_puntos = k_p\n",
    "\n",
    "    #RETORNO DE DATOS\n",
    "    return k_puntos, clusters\n",
    "\n",
    "def load(k, redux,redux_type):\n",
    "\n",
    "    #PROCESAMIENTO DE DATOS\n",
    "    borrarcomillas = lambda x: x.replace('\"', '')\n",
    "    names = np.genfromtxt('clase.csv', dtype=None, delimiter=\",\", skip_header=1, usecols=1, encoding='utf-8', converters={1: borrarcomillas})\n",
    "    data = np.genfromtxt('dataset.csv', delimiter=\",\")\n",
    "\n",
    "    #REDUCCION DE DIMENSIONALIDAD\n",
    "    svd = TruncatedSVD(n_components=redux)\n",
    "    pca = PCA(n_components=redux)\n",
    "    datos = data\n",
    "    if redux_type == 0:\n",
    "        datos = svd.fit(data).transform(data)\n",
    "    elif redux_type == 1:\n",
    "        datos = pca.fit(data).transform(data)\n",
    "\n",
    "\n",
    "    '''\n",
    "    maximo = 0\n",
    "    minimo = 9999999999999\n",
    "    for i in range(len(datos)):\n",
    "        for j in range(len(datos[i])):\n",
    "            #datos[i][j] = 1.08**datos[i][j]\n",
    "            if maximo < datos[i][j]:\n",
    "                maximo = datos[i][j]\n",
    "            if minimo > datos[i][j]:\n",
    "                minimo = datos[i][j]\n",
    "    '''\n",
    "\n",
    "    #VARIABLES UTILES\n",
    "    n_datos = len(datos)\n",
    "    dim_datos = len(datos[0])\n",
    "    \n",
    "    #CARGA DE PUNTOS\n",
    "    puntos = []\n",
    "    k_puntos = []\n",
    "    for i in range(n_datos):\n",
    "        puntos.append(np.array(datos[i]))\n",
    "\n",
    "    #OBTENCIÓN DE PROMEDIOS DE CADA FEATURE Y SU DESVIACIÓN ESTANDAR\n",
    "\n",
    "    data_ = datos.transpose()\n",
    "    sd = []\n",
    "    promedios = []\n",
    "    for x in data_:\n",
    "        sd.append(np.std(x))\n",
    "        promedios.append(np.mean(x))\n",
    "    \n",
    "\n",
    "    #GENERACION DE LOS K PUNTOS PARA LOS CLUSTERES USANDO UN RANGO DETERMINADO POR EL PROMEDIO + Q*DESVIACION ESTANDAR DONDE Q ES UN NUMERO ARBITRARIO PERO QUE TENGA SENTIDO\n",
    "    for i in range(k):\n",
    "        aux = []\n",
    "        for i in range(dim_datos):\n",
    "            aux.append(random.randint(round(promedios[i]-2*sd[i]),round(promedios[i]+2*sd[i])))\n",
    "        k_puntos.append(np.array(aux))\n",
    "\n",
    "  \n",
    "    #RETORNO DE LOS K PUNTOS PARA LOS CLUSTERES, LOS PUNTOS CARGADOS, LA DIMENSION DE LOS DATOS Y LOS LABELS CORRECTOS\n",
    "    return k_puntos, puntos, dim_datos, names\n",
    "\n",
    "\n",
    "\n",
    "def kmeans(k, epoch, redux, redux_type):\n",
    "\n",
    "    #CARGA DE DATOS\n",
    "    k_puntos, puntos, dim_datos, names = load(k, redux, redux_type)\n",
    "\n",
    "    \n",
    "    #MECANISMO PARA IMPRIMIR PORCENTAJE ACTUAL DE LA OPERACIÓN Y SABER CUANTO APROXIMADAMENTE FALTA PARA QUE TERMINE EL ALGORITMO\n",
    "    suma = 0\n",
    "    if(epoch >= 100):\n",
    "        per = epoch / 100\n",
    "        suma = 1\n",
    "    else:\n",
    "        per = 100/epoch\n",
    "        suma = per\n",
    "        \n",
    "    tiempo = 0\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    #LLAMADA AL ALGORITMO iteration() CON ITERACIONES DE REFINAMIENTO=EPOCH E IMPRESIÓN DE PORCENTAJE ACTUAL DE EJECUCIÓN DEL ALGORITMO\n",
    "    clusters = []\n",
    "    for i in range(epoch):\n",
    "        if tiempo >= per:\n",
    "            print(str(count)+'%')\n",
    "            count += suma\n",
    "            tiempo = 0\n",
    "        k_puntos,clusters = iteration(k_puntos, puntos, dim_datos)\n",
    "        tiempo+=suma\n",
    "    print(str(100)+'%')  \n",
    "            \n",
    "    \n",
    "    #FORMATEO CORRECTO DEL RESULTADO PARA DISPLAY \n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        aux = []\n",
    "        for j in range(len(clusters)):\n",
    "            if clusters[j] == i:\n",
    "                aux.append(names[j])\n",
    "        results.append(aux)\n",
    "    \n",
    "    #IMPRESIÓN DE RESULTADOS\n",
    "    count = 0\n",
    "    for i in results:\n",
    "        print('Cluster:', count, '\\n')\n",
    "        print(i)\n",
    "        count +=1\n",
    "\n",
    "    #RETORNO DE RESULTADOS\n",
    "    return clusters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MAIN\n",
    "# k = cantidad de clusteres\n",
    "# epochs = cantidad de iteraciones de 'refinación'\n",
    "# redux = cantidad de dimensiones finales que tendrá el dataset (pca puede quejarse si le pones mas de N, siendo N la cantidad de datos)\n",
    "# redux_type 0 = svd, 1 = pca\n",
    "# kmeans() retorna un arreglo arr[] en donde cada posición i representa el elemento i del dataset[] ya reducido y procesado, y arr[i] contiene el número de cluster que se le ha asignado. El arreglo names[i] contiene el label real del elemento dataset[i]\n",
    "# la reducción de dimensionalidad se hace en la función load(), dejaré un indicador para que sea evidente\n",
    "k = 7\n",
    "epochs = 100\n",
    "redux = 10\n",
    "redux_type = 0 \n",
    "kmeans(k,epochs, redux, redux_type)\n",
    "\n"
   ]
  }
 ]
}