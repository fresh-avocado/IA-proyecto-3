{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import random\n",
    "from scipy.stats import multivariate_normal\n",
    "#ORDEN DE REVISIÓN SUGERIDO: \n",
    "# 1) kmeans()\n",
    "# 2) load()\n",
    "# 3) iteration()\n",
    "# 4) distance()\n",
    "#EL MAIN SE ENCUENTRA AL PIE DEL ARCHIVO\n",
    "\n",
    "\n",
    "def distance(a, b):\n",
    "    #DISTANCIA EUCLIDIANA N DIMENSIONAL\n",
    "    aux = np.sum(np.square(a - b))\n",
    "    return np.sqrt(aux)\n",
    "\n",
    "\n",
    "def iteration(k_puntos, puntos, dim_datos):\n",
    "    #CALCULO DE DISTANCIA DE CADA DATO CON LOS K CENTROIDES DE LOS CLUSTERES Y ASIGNAMIENTO DEL CLUSTER MAS CERCANO PARA CADA DATO\n",
    "    clusters = []\n",
    "    for idx, punto in enumerate(puntos):\n",
    "        d = 99999999999991\n",
    "        ki = 0\n",
    "        for idx_k, k_i in enumerate(k_puntos):\n",
    "            if(d > distance(punto, k_i)):\n",
    "                d = distance(punto, k_i)\n",
    "                ki = idx_k\n",
    "        clusters.append(ki)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #CONTABILIZACION DE ELEMENTOS POR CADA CLUSTER Y SUMA DE FEATURES PARA CADA UNO DE ELLOS\n",
    "    aux = [0] * dim_datos\n",
    "    means = [np.array(aux)] * len(puntos)\n",
    "    count = [0] * len(puntos)\n",
    "    for i in range(0,len(puntos)):\n",
    "        idx = clusters[i]\n",
    "        means[idx] = np.add(puntos[i], means[idx])\n",
    "        count[idx] += 1\n",
    "\n",
    "\n",
    "    #CÁLCULO DE LA MEDIA PARA CADA CLUSTER Y REASIGNACIÓN DE LOS K CENTROIDES\n",
    "    k_p = []\n",
    "    for j in range(len(means)):\n",
    "        # print(means[i])\n",
    "        # print(count[i])\n",
    "        # break\n",
    "        if(count[j] != 0):\n",
    "            #print('a')\n",
    "            for i in range(len(means[j])):\n",
    "                means[j][i] = means[j][i]/count[j]\n",
    "            k_p.append(np.array(means[j]))\n",
    "    k_puntos = k_p\n",
    "\n",
    "    #RETORNO DE DATOS\n",
    "    return k_puntos, clusters\n",
    "\n",
    "def load(k, redux,redux_type, q):\n",
    "\n",
    "    #PROCESAMIENTO DE DATOS\n",
    "    borrarcomillas = lambda x: x.replace('\"', '')\n",
    "    names = np.genfromtxt('clase.csv', dtype=None, delimiter=\",\", skip_header=1, usecols=1, encoding='utf-8', converters={1: borrarcomillas})\n",
    "    data = np.genfromtxt('dataset.csv', delimiter=\",\")\n",
    "\n",
    "    #REDUCCION DE DIMENSIONALIDAD\n",
    "    svd = TruncatedSVD(n_components=redux)\n",
    "    pca = PCA(n_components=redux)\n",
    "    datos = data\n",
    "    if redux_type == 0:\n",
    "        datos = svd.fit(data).transform(data)\n",
    "    elif redux_type == 1:\n",
    "        datos = pca.fit(data).transform(data)\n",
    "\n",
    "\n",
    "    '''\n",
    "    maximo = 0\n",
    "    minimo = 9999999999999\n",
    "    for i in range(len(datos)):\n",
    "        for j in range(len(datos[i])):\n",
    "            #datos[i][j] = 1.08**datos[i][j]\n",
    "            if maximo < datos[i][j]:\n",
    "                maximo = datos[i][j]\n",
    "            if minimo > datos[i][j]:\n",
    "                minimo = datos[i][j]\n",
    "    '''\n",
    "\n",
    "    #VARIABLES UTILES\n",
    "    n_datos = len(datos)\n",
    "    dim_datos = len(datos[0])\n",
    "    \n",
    "    #CARGA DE PUNTOS\n",
    "    puntos = []\n",
    "    k_puntos = []\n",
    "    for i in range(n_datos):\n",
    "        puntos.append(np.array(datos[i]))\n",
    "\n",
    "    #OBTENCIÓN DE PROMEDIOS DE CADA FEATURE Y SU DESVIACIÓN ESTANDAR\n",
    "\n",
    "    data_ = datos.transpose()\n",
    "    sd = []\n",
    "    promedios = []\n",
    "    for x in data_:\n",
    "        sd.append(np.std(x))\n",
    "        promedios.append(np.mean(x))\n",
    "    \n",
    "\n",
    "    #GENERACION DE LOS K PUNTOS PARA LOS CLUSTERES USANDO UN RANGO DETERMINADO POR EL PROMEDIO + Q*DESVIACION ESTANDAR DONDE Q ES UN NUMERO ARBITRARIO PERO QUE TENGA SENTIDO\n",
    "    np.random.seed(44)\n",
    "    for i in range(k):\n",
    "        aux = []\n",
    "        for i in range(dim_datos):\n",
    "            aux.append(np.random.randint(round(promedios[i]-q*sd[i]),round(promedios[i]+q*sd[i])))\n",
    "        k_puntos.append(np.array(aux))\n",
    "\n",
    "  \n",
    "    #RETORNO DE LOS K PUNTOS PARA LOS CLUSTERES, LOS PUNTOS CARGADOS, LA DIMENSION DE LOS DATOS Y LOS LABELS CORRECTOS\n",
    "    return k_puntos, puntos, dim_datos, names\n",
    "\n",
    "\n",
    "\n",
    "def kmeans(k, epoch, redux, redux_type, q):\n",
    "\n",
    "    #CARGA DE DATOS\n",
    "    k_puntos, puntos, dim_datos, names = load(k, redux, redux_type, q)\n",
    "\n",
    "    \n",
    "    #MECANISMO PARA IMPRIMIR PORCENTAJE ACTUAL DE LA OPERACIÓN Y SABER CUANTO APROXIMADAMENTE FALTA PARA QUE TERMINE EL ALGORITMO\n",
    "    suma = 0\n",
    "    if(epoch >= 100):\n",
    "        per = epoch / 100\n",
    "        suma = 1\n",
    "    else:\n",
    "        per = 100/epoch\n",
    "        suma = per\n",
    "        \n",
    "    tiempo = 0\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    #LLAMADA AL ALGORITMO iteration() CON ITERACIONES DE REFINAMIENTO=EPOCH E IMPRESIÓN DE PORCENTAJE ACTUAL DE EJECUCIÓN DEL ALGORITMO\n",
    "    clusters = []\n",
    "    for i in range(epoch):\n",
    "        if tiempo >= per:\n",
    "            # print(str(count)+'%')\n",
    "            count += suma\n",
    "            tiempo = 0\n",
    "        k_puntos,clusters = iteration(k_puntos, puntos, dim_datos)\n",
    "        tiempo+=suma\n",
    "    # print(str(100)+'%')  \n",
    "            \n",
    "    \n",
    "    #FORMATEO CORRECTO DEL RESULTADO PARA DISPLAY \n",
    "    results = []\n",
    "    objects = []\n",
    "\n",
    "\n",
    "\n",
    "    cluster_map = {}\n",
    "    cluster_features_means = {}\n",
    "    for i in range(k):\n",
    "        aux = []\n",
    "        elements_in_cluster = []\n",
    "        for j in range(len(clusters)):\n",
    "            if clusters[j] == i:\n",
    "                aux.append(puntos[j])\n",
    "                elements_in_cluster.append(puntos[j])\n",
    "        \n",
    "        #estoy en el cluster i con elements_in_clusters elementos en dicho cluster\n",
    "        features_prom = []\n",
    "        for x in range(dim_datos):\n",
    "            prom = 0\n",
    "            for y in range(len(elements_in_cluster)):\n",
    "                prom += elements_in_cluster[y][x]\n",
    "\n",
    "            prom = prom / dim_datos\n",
    "\n",
    "            features_prom.append(prom)\n",
    "\n",
    "\n",
    "        cluster_map[i] = np.array(aux)\n",
    "        cluster_features_means[i] = np.array(features_prom)\n",
    "\n",
    "    for i in range(k):\n",
    "        aux = []\n",
    "        aux1 = []\n",
    "        for j in range(len(clusters)):\n",
    "            if clusters[j] == i:\n",
    "                aux.append(names[j])\n",
    "                \n",
    "        results.append(aux)\n",
    "    \n",
    "    #IMPRESIÓN DE RESULTADOS\n",
    "    count = 0\n",
    "    for i in results:\n",
    "        print('Cluster:', count, '\\n')\n",
    "        print(i)\n",
    "        count +=1\n",
    "\n",
    "    #RETORNO DE RESULTADOS\n",
    "\n",
    "    return clusters, cluster_map, cluster_features_means, np.array(puntos), names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MAIN\n",
    "# k = cantidad de clusteres\n",
    "# epochs = cantidad de iteraciones de 'refinación'\n",
    "# redux = cantidad de dimensiones finales que tendrá el dataset (pca puede quejarse si le pones mas de N, siendo N la cantidad de datos)\n",
    "# redux_type 0 = svd, 1 = pca\n",
    "# q = cantidad de veces que se suma y resta la desviación estandar a la media para obtener un rango de generación para las features de los centroides aleatorios. \n",
    "# rango para cada feature = [ft_mean-q*ft_sd, ft_mean+q*ft_sd]\n",
    "\n",
    "# kmeans() retorna un arreglo arr[] en donde cada posición i representa el elemento i del dataset[] ya reducido y procesado, y arr[i] contiene el número de cluster que se le ha asignado. El arreglo names[i] contiene el label real del elemento dataset[i]\n",
    "# la reducción de dimensionalidad se hace en la función load(), dejaré un indicador para que sea evidente\n",
    "\n",
    "\n",
    "#print(cm[0])\n",
    "# print(cfm[0])\n",
    "#PROBLEMAS ENCONTRADOS:\n",
    "\n",
    "#CLUSTERS CON DOS CATEGORÍAS\n",
    "#Posible respuesta: Algunas categorías de labels no aparecen en un cluster propio, puede deberse a que la naturaleza de los features de dicha categoría sean demasiado similares a otra categoría, por lo que se clusteriza en el mismo sitio o que en la reducción de dimensionalidad, se hayan acortado diferencias entre dos categorías y sus labels, lo que resulta en la primera observación.\n",
    "\n",
    "#MAS DE UN CLUSTER PARA UNA CATEGORÍA\n",
    "#Posible respuesta: Los features de los datos de una misma categoría tienen tendencia a diverger, con la reducción de dimensionalidad, en dos o más grupos diferentes o (la que creo que es) dos centroides aleatorios a1 y a2 cayeron muy cerca entre sí y se formaron dos miniclusters que representan lo mismo. NOTA: como el rango de generación de cada feature para los centroides aleatorios es media+-q*desviacion estandar, si sucede la primera observación, quiere decir que los rangos probablemente resultaron muy pequeños, por lo que se generaron puntos con features similares y habría que aumentar el q para dar mas holgura.\n",
    "\n",
    "#CLUSTERES VACÍOS\n",
    "#Posible respuesta: Se generó un centroide tan lejano que no resultó ser el más cercano para ningún dato, habría que evaluar disminuir el q para disminuir la holgura de generación y crear centroides más cercanos a los puntos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_medias(gammas, N, n_samples, n_classes, k_clusters):\n",
    "  medias = np.zeros((k_clusters, n_classes))\n",
    "  for i in range(k_clusters):\n",
    "    media = np.full((n_classes,), 0, dtype='float64')\n",
    "    for n in range(n_samples):\n",
    "      media += gammas[n,i]*dataset[n,]\n",
    "    media = media/N[i]\n",
    "    medias[i,] += media\n",
    "  return medias\n",
    "\n",
    "def new_covarianzas(gammas, medias, N, n_samples, n_classes, k_clusters):\n",
    "  covarianzas = np.zeros((k_clusters, n_classes, n_classes))\n",
    "\n",
    "  for i in range(k_clusters):\n",
    "    covarianza = np.zeros((n_classes, n_classes))\n",
    "    for n in range(n_samples):\n",
    "      columna = dataset[n,] - medias[i,]\n",
    "      columna = columna.reshape(n_classes, 1)\n",
    "      fila = columna.reshape(1, n_classes)\n",
    "      covarianza += gammas[n,i]*(columna@fila)\n",
    "    covarianzas[i,:,:] += covarianza/N[i]\n",
    "\n",
    "  return covarianzas\n",
    "\n",
    "def new_N(gammas, k_clusters, n_samples):\n",
    "  N = np.zeros((k_clusters,))\n",
    "  for i in range(k_clusters):\n",
    "    val = 0.0\n",
    "    for n in range(n_samples):\n",
    "      val += gammas[n,i]\n",
    "    N[i,] += val\n",
    "\n",
    "  return N\n",
    "\n",
    "def new_pis(N, n_samples, k_clusters):\n",
    "  pis = np.zeros((k_clusters,))\n",
    "  for i in range(k_clusters):\n",
    "    pis[i,] += N[i]/n_samples\n",
    "  return pis\n",
    "\n",
    "epsilon = 0.0001\n",
    "\n",
    "def new_responsibilities(pis, medias, covarianzas, n_samples, k_clusters):\n",
    "  gammas = np.zeros((n_samples, k_clusters))\n",
    "  for n in range(n_samples):\n",
    "    for k in range(k_clusters):\n",
    "      numerador = pis[k,]*(multivariate_normal.pdf(dataset[n,], mean=medias[k,], cov=covarianzas[k,:,:], allow_singular=True)+epsilon)\n",
    "      # print('muultivariate sin epsilon: ', multivariate_normal.pdf(dataset[n,], mean=medias[k,], cov=covarianzas[k,:,:], allow_singular=True))\n",
    "      # print('pis[k,]: ', pis[k,])\n",
    "      # print('covs: ', covarianzas[k,:,:])\n",
    "      # print('medias: ', medias[k,])\n",
    "      # print('numerador: ', numerador)\n",
    "      denominador = 0.0\n",
    "      for _k in range(k_clusters):\n",
    "        denominador += pis[_k,]*(multivariate_normal.pdf(dataset[n,], mean=medias[_k,], cov=covarianzas[_k,:,:], allow_singular=True)+epsilon)\n",
    "      # print('denominador: ', denominador)\n",
    "      res = numerador/denominador\n",
    "      gammas[n, k] += res\n",
    "  return gammas\n",
    "\n",
    "def log_likelihood(k_clusters, medias, covarianzas, n_samples, pis):\n",
    "  prob = 0\n",
    "  for n in range(n_samples):\n",
    "    res = 0\n",
    "    for k in range(k_clusters):\n",
    "      res += pis[k,]*(multivariate_normal.pdf(dataset[n,], mean=medias[k,], cov=covarianzas[k,:,:], allow_singular=True)+epsilon)\n",
    "    # print('res: ', res)\n",
    "    res = np.log(res)\n",
    "    prob += res\n",
    "  return prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0 \n",
      "\n",
      "['cerebellum', 'cerebellum', 'liver', 'liver', 'kidney', 'kidney']\n",
      "Cluster: 1 \n",
      "\n",
      "['colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver']\n",
      "Cluster: 2 \n",
      "\n",
      "['kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium']\n",
      "Cluster: 3 \n",
      "\n",
      "['placenta', 'placenta', 'placenta', 'placenta', 'placenta', 'placenta']\n",
      "Cluster: 4 \n",
      "\n",
      "['hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "Cluster: 5 \n",
      "\n",
      "['cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "Cluster: 6 \n",
      "\n",
      "[]\n",
      "[[-33.36763725 -27.60139899  50.92606134   4.52479241  21.69051214\n",
      "   -5.2303036  -22.93771992  -6.09637173 -17.68023392 -14.79085524\n",
      "    2.34063559  19.66964318 -11.49638095 -16.37374216  15.02515693\n",
      "    7.27066446  -1.41730369  10.59528877   7.19522363  -4.29326496]\n",
      " [-41.48310326 -36.99128403  36.45687313  -1.04735133  10.5313412\n",
      "   -0.73370818  -4.64290615  32.00931467  -1.33357703 -16.57177367\n",
      "   -4.51354845  -7.91011356  -1.20527112 -22.05570948  -3.83304684\n",
      "   14.9302446    8.2587389    5.16248554  21.91954181  -3.66613039]]\n",
      "(189, 20)\n"
     ]
    }
   ],
   "source": [
    "k = 7\n",
    "epochs = 300\n",
    "redux = 20\n",
    "redux_type = 1 # NOTE: comentar sobre el resultado de '1'\n",
    "q = 2\n",
    "#cm = cluster map, cfm = cluster feature means, dt = dataset\n",
    "clusters, cm, cfm, dataset, names = kmeans(k, epochs, redux, redux_type, q)\n",
    "print(dataset[0:2,])\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pis:  [0.14285714 0.14285714 0.14285714 0.14285714 0.14285714 0.14285714\n",
      " 0.14285714]\n",
      "means:  [[-126.56868528 -149.2535791  -137.60165527   22.01701278  -26.15068852\n",
      "   -62.00871507  -21.14334221    4.10429168    2.493903     -5.35016006]\n",
      " [ 242.21393962   32.52003806   53.34063548  133.84744816  -49.16640154\n",
      "    -2.70021218   -4.79861977   -5.23714772   18.69728698    1.65385659]\n",
      " [ -80.764433    -16.50981299   22.7468173    -3.86777301   43.96616355\n",
      "    64.80324034  -53.28307774  -35.59198966    5.40896303   -4.28802911]\n",
      " [-113.51502314 -111.9446045   129.09070813  -21.12102157   38.07832988\n",
      "    12.29904356   70.21580507   21.44885281  -17.65759117    5.14330077]\n",
      " [ 274.80396767   45.65028669  -59.68983449 -115.73223201   34.20866328\n",
      "     2.17874471    2.23777398   11.49800163   -5.73933824   -1.67528766]\n",
      " [-196.16976587  199.53767183   -7.88667114  -15.14343435  -40.93606665\n",
      "   -14.57210136    6.77146068    3.77799126   -3.2032236     4.51631946]\n",
      " [   0.            0.            0.            0.            0.\n",
      "     0.            0.            0.            0.            0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/core/_methods.py:163: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n",
      "/Users/gabrielspranger/Library/Python/3.6/lib/python/site-packages/ipykernel_launcher.py:18: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/function_base.py:2480: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  c *= np.true_divide(1, fact)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/function_base.py:2480: RuntimeWarning: invalid value encountered in multiply\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-b54286aba042>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'means: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovarianzas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprev_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-8fd881101e13>\u001b[0m in \u001b[0;36mlog_likelihood\u001b[0;34m(k_clusters, medias, covarianzas, n_samples, pis)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmedias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcovarianzas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_singular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;31m# print('res: ', res)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36mpdf\u001b[0;34m(self, x, mean, cov, allow_singular)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_quantiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mpsd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_PSD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_singular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_pdet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_squeeze_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Note that eigh takes care of array conversion, chkfinite,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m# and assertion that the matrix is square.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_eigvalsh_to_eps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/linalg/decomp.py\u001b[0m in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite, subset_by_index, subset_by_value, driver)\u001b[0m\n\u001b[1;32m    443\u001b[0m                          ''.format(driver, '\", \"'.join(drv_str[1:])))\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected square \"a\" matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'masked arrays are not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         raise ValueError(\n\u001b[0;32m--> 486\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "n_samples = np.size(dataset, axis=0)\n",
    "# cm[cluster_num] -> array de arrays (puntos)\n",
    "# cfm[cluster_num] -> media del cluster 'cluster_num'\n",
    "# print(dataset)\n",
    "means = np.zeros((k, redux))\n",
    "\n",
    "# print('cfm.items(): ', cfm[0])\n",
    "\n",
    "for idx, (key, value) in enumerate(cfm.items()):\n",
    "  means[idx,] += value\n",
    "\n",
    "covarianzas = np.zeros((k, redux, redux))\n",
    "\n",
    "# print('cm.item(): ', cm[0])\n",
    "\n",
    "for idx, (key, value) in enumerate(cm.items()):\n",
    "  # print('calculando covarianza...')\n",
    "  cova = np.cov(value, rowvar=False)\n",
    "  # print('cov shape: ', cova.shape)\n",
    "  # print('cov: ', cova)\n",
    "  covarianzas[idx,:,:] += cova # NOTE: acabo de intentar esto (abs)\n",
    "\n",
    "pis = np.full((k,), 1/k, dtype='float64')\n",
    "\n",
    "print('pis: ', pis)\n",
    "# print('covarianzas: ', covarianzas)\n",
    "print('means: ', means)\n",
    "\n",
    "likelihood = log_likelihood(k, means, covarianzas, n_samples, pis)\n",
    "prev_likelihood = 1000\n",
    "\n",
    "print('likelihood: ', likelihood)\n",
    "\n",
    "gammas = np.array([])\n",
    "N = np.array([])\n",
    "\n",
    "threshold = 0.000001\n",
    "\n",
    "i = 0\n",
    "\n",
    "while (True):\n",
    "  # if abs(prev_likelihood-likelihood) < threshold:\n",
    "  if prev_likelihood == likelihood:\n",
    "    print('diff likelihood: ', abs(prev_likelihood-likelihood))\n",
    "    break\n",
    "  prev_likelihood = likelihood\n",
    "  print('N: ', N)\n",
    "  # TODO: revisar si estas variables de actualizan o no\n",
    "  gammas = new_responsibilities(pis, means, covarianzas, n_samples, k)\n",
    "  N = new_N(gammas, k, n_samples)\n",
    "  means = new_medias(gammas, N, n_samples, redux, k)\n",
    "  covarianzas = new_covarianzas(gammas, means, N, n_samples, redux, k)\n",
    "  pis = new_pis(N, n_samples, k)\n",
    "  # print('gammas: ', gammas)\n",
    "  # print('k: ', k)\n",
    "  # print('means: ', means)\n",
    "  # print('covarianzas: ', covarianzas)\n",
    "  # print('n_samples: ', n_samples)\n",
    "  # print('pis: ', pis)\n",
    "  likelihood = log_likelihood(k, means, covarianzas, n_samples, pis)\n",
    "  print('likelihood: ', likelihood)\n",
    "  i += 1\n",
    "\n",
    "print('final likelihood: ', likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION: intento fallido de GMM\n",
    "\n",
    "# agrupamiento = {}\n",
    "\n",
    "# for i in range(n_samples):\n",
    "#   max_prob = -1\n",
    "#   cluster = -1\n",
    "#   for j in range(k):\n",
    "#     z = gammas[i, j]\n",
    "#     if z > max_prob:\n",
    "#       max_prob = z\n",
    "#       cluster = j\n",
    "#   if agrupamiento.get(cluster) == None:\n",
    "#     agrupamiento[cluster] = [dataset[i,]]\n",
    "#   else:\n",
    "#     agrupamiento[cluster].append(dataset[i,])\n",
    "\n",
    "# # TODO: confirmar que cada punto dentro del cluster\n",
    "# # TODO: terminar de programar como si ya estuviera\n",
    "# # NOTE: con la ayuda de 'names'\n",
    "\n",
    "# print(gammas)\n",
    "\n",
    "# print(agrupamiento.keys())\n",
    "# # print(len(agrupamiento[0]))\n",
    "# print(agrupamiento)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0:\n",
      " ['kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney']\n",
      "valor más común:  kidney\n",
      "#ocurrencias valor más común:  19\n",
      "cluster size:  19\n",
      "=================\n",
      "cluster 1:\n",
      " ['kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney']\n",
      "valor más común:  kidney\n",
      "#ocurrencias valor más común:  20\n",
      "cluster size:  20\n",
      "=================\n",
      "cluster 2:\n",
      " ['hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "valor más común:  hippocampus\n",
      "#ocurrencias valor más común:  31\n",
      "cluster size:  36\n",
      "=================\n",
      "cluster 3:\n",
      " ['cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "valor más común:  cerebellum\n",
      "#ocurrencias valor más común:  33\n",
      "cluster size:  33\n",
      "=================\n",
      "cluster 4:\n",
      " ['colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'placenta', 'placenta', 'placenta', 'placenta', 'placenta', 'placenta']\n",
      "valor más común:  colon\n",
      "#ocurrencias valor más común:  34\n",
      "cluster size:  40\n",
      "=================\n",
      "cluster 5:\n",
      " ['liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver']\n",
      "valor más común:  liver\n",
      "#ocurrencias valor más común:  26\n",
      "cluster size:  26\n",
      "=================\n",
      "cluster 6:\n",
      " ['endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium']\n",
      "valor más común:  endometrium\n",
      "#ocurrencias valor más común:  15\n",
      "cluster size:  15\n",
      "=================\n",
      "log likelihood:  -40.503922335270985\n",
      "cluster 0:\n",
      " ['kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney']\n",
      "valor más común:  kidney\n",
      "#ocurrencias valor más común:  21\n",
      "cluster size:  21\n",
      "=================\n",
      "cluster 1:\n",
      " ['kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney']\n",
      "valor más común:  kidney\n",
      "#ocurrencias valor más común:  18\n",
      "cluster size:  18\n",
      "=================\n",
      "cluster 2:\n",
      " ['hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "valor más común:  hippocampus\n",
      "#ocurrencias valor más común:  31\n",
      "cluster size:  36\n",
      "=================\n",
      "cluster 3:\n",
      " ['cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "valor más común:  cerebellum\n",
      "#ocurrencias valor más común:  33\n",
      "cluster size:  33\n",
      "=================\n",
      "cluster 4:\n",
      " ['colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon']\n",
      "valor más común:  colon\n",
      "#ocurrencias valor más común:  34\n",
      "cluster size:  34\n",
      "=================\n",
      "cluster 5:\n",
      " ['liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver']\n",
      "valor más común:  liver\n",
      "#ocurrencias valor más común:  26\n",
      "cluster size:  26\n",
      "=================\n",
      "cluster 6:\n",
      " ['endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'placenta', 'placenta', 'placenta', 'placenta', 'placenta', 'placenta']\n",
      "valor más común:  endometrium\n",
      "#ocurrencias valor más común:  15\n",
      "cluster size:  21\n",
      "=================\n",
      "log likelihood:  -71.28626784600687\n",
      "cluster 0:\n",
      " ['kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney']\n",
      "valor más común:  kidney\n",
      "#ocurrencias valor más común:  37\n",
      "cluster size:  37\n",
      "=================\n",
      "cluster 1:\n",
      " ['hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'cerebellum', 'cerebellum', 'hippocampus', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "valor más común:  hippocampus\n",
      "#ocurrencias valor más común:  31\n",
      "cluster size:  41\n",
      "=================\n",
      "cluster 2:\n",
      " ['cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "valor más común:  cerebellum\n",
      "#ocurrencias valor más común:  26\n",
      "cluster size:  26\n",
      "=================\n",
      "cluster 3:\n",
      " ['colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon']\n",
      "valor más común:  colon\n",
      "#ocurrencias valor más común:  34\n",
      "cluster size:  34\n",
      "=================\n",
      "cluster 4:\n",
      " ['liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver']\n",
      "valor más común:  liver\n",
      "#ocurrencias valor más común:  26\n",
      "cluster size:  26\n",
      "=================\n",
      "cluster 5:\n",
      " ['cerebellum', 'cerebellum', 'kidney', 'kidney', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium']\n",
      "valor más común:  endometrium\n",
      "#ocurrencias valor más común:  15\n",
      "cluster size:  19\n",
      "=================\n",
      "cluster 6:\n",
      " ['placenta', 'placenta', 'placenta', 'placenta', 'placenta', 'placenta']\n",
      "valor más común:  placenta\n",
      "#ocurrencias valor más común:  6\n",
      "cluster size:  6\n",
      "=================\n",
      "log likelihood:  -70.52379078966737\n",
      "cluster 0:\n",
      " ['kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'hippocampus', 'hippocampus', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'cerebellum', 'cerebellum', 'cerebellum', 'liver', 'liver', 'kidney', 'kidney', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'endometrium', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "valor más común:  kidney\n",
      "#ocurrencias valor más común:  22\n",
      "cluster size:  51\n",
      "=================\n",
      "cluster 1:\n",
      " ['kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney', 'kidney']\n",
      "valor más común:  kidney\n",
      "#ocurrencias valor más común:  17\n",
      "cluster size:  17\n",
      "=================\n",
      "cluster 2:\n",
      " ['hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus', 'hippocampus']\n",
      "valor más común:  hippocampus\n",
      "#ocurrencias valor más común:  29\n",
      "cluster size:  29\n",
      "=================\n",
      "cluster 3:\n",
      " ['cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum', 'cerebellum']\n",
      "valor más común:  cerebellum\n",
      "#ocurrencias valor más común:  28\n",
      "cluster size:  28\n",
      "=================\n",
      "cluster 4:\n",
      " ['colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon', 'colon']\n",
      "valor más común:  colon\n",
      "#ocurrencias valor más común:  34\n",
      "cluster size:  34\n",
      "=================\n",
      "cluster 5:\n",
      " ['liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver', 'liver']\n",
      "valor más común:  liver\n",
      "#ocurrencias valor más común:  24\n",
      "cluster size:  24\n",
      "=================\n",
      "cluster 6:\n",
      " ['placenta', 'placenta', 'placenta', 'placenta', 'placenta', 'placenta']\n",
      "valor más común:  placenta\n",
      "#ocurrencias valor más común:  6\n",
      "cluster size:  6\n",
      "=================\n",
      "log likelihood:  -74.46527507168628\n"
     ]
    }
   ],
   "source": [
    "# SECTION: con librerías\n",
    "\n",
    "from sklearn import mixture\n",
    "import time as clock\n",
    "from collections import Counter\n",
    "\n",
    "borrarcomillas = lambda x: x.replace('\"', '')\n",
    "names = np.genfromtxt('clase.csv', dtype=None, delimiter=\",\", skip_header=1, usecols=1, encoding='utf-8', converters={1: borrarcomillas})\n",
    "# print(names)\n",
    "\n",
    "# TODO: experimentar con 'covariance_type' e 'init_params'\n",
    "\n",
    "labels = np.unique(names)\n",
    "\n",
    "covariance_types = ['full', 'tied', 'diag', 'spherical']\n",
    "init_parameters = ['random', 'kmeans']\n",
    "tiempo_i = 0\n",
    "tiempo_f = 0\n",
    "\n",
    "def get_cluster_accuracy(gmm, labels, cov_type, t):\n",
    "  # print('(tejido_predecido, tejido_verdadero)')\n",
    "  agrupacion = {}\n",
    "  probabilidad_conjunta = gmm.score(dataset, names)\n",
    "  for idx in range(n_samples):\n",
    "    prediction = gmm.predict(np.array([dataset[idx,]]))[0,]\n",
    "    # tejido_predecido = labels[prediction,]\n",
    "    tejido_predecido = prediction\n",
    "    tejido_verdadero = names[idx,]\n",
    "    # print(tejido_predecido, tejido_verdadero)\n",
    "    if agrupacion.get(tejido_predecido) == None:\n",
    "      agrupacion[tejido_predecido] = [tejido_verdadero]\n",
    "    else:\n",
    "      agrupacion[tejido_predecido].append(tejido_verdadero)\n",
    "\n",
    "  cluster_names, occurrences, cluster_sizes = [], [], []\n",
    "  \n",
    "  for idx, (key, value) in enumerate(agrupacion.items()):\n",
    "    print('cluster ' + str(idx) + ':\\n', value)\n",
    "    counter = Counter(value)\n",
    "    mas_comunes = counter.most_common(1)\n",
    "    print('valor más común: ', mas_comunes[0][0])\n",
    "    print('#ocurrencias valor más común: ', mas_comunes[0][1])\n",
    "    cluster_names.append(mas_comunes[0][0])\n",
    "    occurrences.append(mas_comunes[0][1])\n",
    "    cluster_sizes.append(len(value))\n",
    "    print('cluster size: ', len(value))\n",
    "    print('=================')\n",
    "  print('log likelihood: ', probabilidad_conjunta)\n",
    "\n",
    "  no_identificados = set(labels) - set(cluster_names)\n",
    "  no_identificados_str = ''\n",
    "  \n",
    "  for idx, no_identificado in enumerate(no_identificados):\n",
    "    if idx != len(no_identificados)-1:\n",
    "      no_identificados_str += str(no_identificado) + ', '\n",
    "    else:\n",
    "      no_identificados_str += str(no_identificado)\n",
    "  \n",
    "  cluster_quantity = '7'\n",
    "\n",
    "  precision_cluster = ''\n",
    "  for idx in range(len(cluster_sizes)):\n",
    "    # cluster_names[idx]: round((occurrences[idx]/cluster_sizes[idx])*100, 2)\\\\%\\\\newline\n",
    "    precision_cluster += cluster_names[idx] + ': ' + str(round((occurrences[idx]/cluster_sizes[idx])*100, 2)) + '\\\\%\\\\newline '\n",
    "\n",
    "  return cov_type + ' & ' + str(round(t, 4)) + ' & ' + no_identificados_str + ' & ' + cluster_quantity + ' & ' + precision_cluster + ' & ' + str(round(probabilidad_conjunta, 2)) + '\\\\\\\\'\n",
    "\n",
    "for c in covariance_types:\n",
    "  gmm = mixture.GaussianMixture(n_components=k, covariance_type=c, init_params='kmeans')\n",
    "  tiempo_i = clock.time()\n",
    "  gmm.fit(dataset, names)\n",
    "  tiempo_f = clock.time()\n",
    "  tiempo_total = tiempo_f - tiempo_i\n",
    "  f = open('testing.txt', 'a')\n",
    "  f.write(get_cluster_accuracy(gmm, labels, c, tiempo_total) + '\\n\\\\hline\\n')\n",
    "  f.close()\n",
    "\n",
    "# gmm = mixture.GaussianMixture(n_components=k, covariance_type='spherical', init_params='kmeans')\n",
    "\n",
    "# gmm.fit(dataset)\n",
    "\n",
    "# print(gmm.get_params())\n",
    "\n",
    "# print(get_cluster_accuracy(gmm, labels, 'spherical', 10))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}