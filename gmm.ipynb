{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import random\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "#ORDEN DE REVISIÓN SUGERIDO: \n",
    "# 1) kmeans()\n",
    "# 2) load()\n",
    "# 3) iteration()\n",
    "# 4) distance()\n",
    "#EL MAIN SE ENCUENTRA AL PIE DEL ARCHIVO\n",
    "\n",
    "\n",
    "def distance(a, b):\n",
    "    #DISTANCIA EUCLIDIANA N DIMENSIONAL\n",
    "    aux = np.sum(np.square(a - b))\n",
    "    return np.sqrt(aux)\n",
    "\n",
    "\n",
    "def iteration(k_puntos, puntos, dim_datos):\n",
    "    #CALCULO DE DISTANCIA DE CADA DATO CON LOS K CENTROIDES DE LOS CLUSTERES Y ASIGNAMIENTO DEL CLUSTER MAS CERCANO PARA CADA DATO\n",
    "    clusters = []\n",
    "    for idx, punto in enumerate(puntos):\n",
    "        d = 99999999999991\n",
    "        ki = 0\n",
    "        for idx_k, k_i in enumerate(k_puntos):\n",
    "            if(d > distance(punto, k_i)):\n",
    "                d = distance(punto, k_i)\n",
    "                ki = idx_k\n",
    "        clusters.append(ki)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #CONTABILIZACION DE ELEMENTOS POR CADA CLUSTER Y SUMA DE FEATURES PARA CADA UNO DE ELLOS\n",
    "    aux = [0] * dim_datos\n",
    "    means = [np.array(aux)] * len(puntos)\n",
    "    count = [0] * len(puntos)\n",
    "    for i in range(0,len(puntos)):\n",
    "        idx = clusters[i]\n",
    "        means[idx] = np.add(puntos[i], means[idx])\n",
    "        count[idx] += 1\n",
    "\n",
    "\n",
    "    #CÁLCULO DE LA MEDIA PARA CADA CLUSTER Y REASIGNACIÓN DE LOS K CENTROIDES\n",
    "    k_p = []\n",
    "    for j in range(len(means)):\n",
    "        # print(means[i])\n",
    "        # print(count[i])\n",
    "        # break\n",
    "        if(count[j] != 0):\n",
    "            #print('a')\n",
    "            for i in range(len(means[j])):\n",
    "                means[j][i] = means[j][i]/count[j]\n",
    "            k_p.append(np.array(means[j]))\n",
    "    k_puntos = k_p\n",
    "\n",
    "    #RETORNO DE DATOS\n",
    "    return k_puntos, clusters\n",
    "\n",
    "def load(k, redux,redux_type, q):\n",
    "\n",
    "    #PROCESAMIENTO DE DATOS\n",
    "    borrarcomillas = lambda x: x.replace('\"', '')\n",
    "    names = np.genfromtxt('clase.csv', dtype=None, delimiter=\",\", skip_header=1, usecols=1, encoding='utf-8', converters={1: borrarcomillas})\n",
    "    data = np.genfromtxt('dataset.csv', delimiter=\",\")\n",
    "\n",
    "    #REDUCCION DE DIMENSIONALIDAD\n",
    "    svd = TruncatedSVD(n_components=redux)\n",
    "    pca = PCA(n_components=redux)\n",
    "    datos = data\n",
    "    if redux_type == 0:\n",
    "        datos = svd.fit(data).transform(data)\n",
    "    elif redux_type == 1:\n",
    "        datos = pca.fit(data).transform(data)\n",
    "\n",
    "\n",
    "    '''\n",
    "    maximo = 0\n",
    "    minimo = 9999999999999\n",
    "    for i in range(len(datos)):\n",
    "        for j in range(len(datos[i])):\n",
    "            #datos[i][j] = 1.08**datos[i][j]\n",
    "            if maximo < datos[i][j]:\n",
    "                maximo = datos[i][j]\n",
    "            if minimo > datos[i][j]:\n",
    "                minimo = datos[i][j]\n",
    "    '''\n",
    "\n",
    "    #VARIABLES UTILES\n",
    "    n_datos = len(datos)\n",
    "    dim_datos = len(datos[0])\n",
    "    \n",
    "    #CARGA DE PUNTOS\n",
    "    puntos = []\n",
    "    k_puntos = []\n",
    "    for i in range(n_datos):\n",
    "        puntos.append(np.array(datos[i]))\n",
    "\n",
    "    #OBTENCIÓN DE PROMEDIOS DE CADA FEATURE Y SU DESVIACIÓN ESTANDAR\n",
    "\n",
    "    data_ = datos.transpose()\n",
    "    sd = []\n",
    "    promedios = []\n",
    "    for x in data_:\n",
    "        sd.append(np.std(x))\n",
    "        promedios.append(np.mean(x))\n",
    "    \n",
    "\n",
    "    #GENERACION DE LOS K PUNTOS PARA LOS CLUSTERES USANDO UN RANGO DETERMINADO POR EL PROMEDIO + Q*DESVIACION ESTANDAR DONDE Q ES UN NUMERO ARBITRARIO PERO QUE TENGA SENTIDO\n",
    "    for i in range(k):\n",
    "        aux = []\n",
    "        for i in range(dim_datos):\n",
    "            aux.append(random.randint(round(promedios[i]-q*sd[i]),round(promedios[i]+q*sd[i])))\n",
    "        k_puntos.append(np.array(aux))\n",
    "\n",
    "  \n",
    "    #RETORNO DE LOS K PUNTOS PARA LOS CLUSTERES, LOS PUNTOS CARGADOS, LA DIMENSION DE LOS DATOS Y LOS LABELS CORRECTOS\n",
    "    return k_puntos, puntos, dim_datos, names\n",
    "\n",
    "\n",
    "\n",
    "def kmeans(k, epoch, redux, redux_type, q):\n",
    "\n",
    "    #CARGA DE DATOS\n",
    "    k_puntos, puntos, dim_datos, names = load(k, redux, redux_type, q)\n",
    "\n",
    "    # print('k_puntos: ', k_puntos)\n",
    "    # print('puntos: ', puntos)\n",
    "\n",
    "    \n",
    "    #MECANISMO PARA IMPRIMIR PORCENTAJE ACTUAL DE LA OPERACIÓN Y SABER CUANTO APROXIMADAMENTE FALTA PARA QUE TERMINE EL ALGORITMO\n",
    "    suma = 0\n",
    "    if(epoch >= 100):\n",
    "        per = epoch / 100\n",
    "        suma = 1\n",
    "    else:\n",
    "        per = 100/epoch\n",
    "        suma = per\n",
    "        \n",
    "    tiempo = 0\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    #LLAMADA AL ALGORITMO iteration() CON ITERACIONES DE REFINAMIENTO=EPOCH E IMPRESIÓN DE PORCENTAJE ACTUAL DE EJECUCIÓN DEL ALGORITMO\n",
    "    clusters = []\n",
    "    for i in range(epoch):\n",
    "        if tiempo >= per:\n",
    "            # print(str(count)+'%')\n",
    "            count += suma\n",
    "            tiempo = 0\n",
    "        k_puntos, clusters = iteration(k_puntos, puntos, dim_datos)\n",
    "        tiempo+=suma\n",
    "    # print(str(100)+'%')  \n",
    "            \n",
    "    \n",
    "    #FORMATEO CORRECTO DEL RESULTADO PARA DISPLAY \n",
    "    results = []\n",
    "    for i in range(k):\n",
    "        aux = []\n",
    "        for j in range(len(clusters)):\n",
    "            if clusters[j] == i:\n",
    "                aux.append(names[j])\n",
    "        results.append(aux)\n",
    "    \n",
    "    #IMPRESIÓN DE RESULTADOS\n",
    "    count = 0\n",
    "    for i in results:\n",
    "        print('Cluster:', count, '\\n')\n",
    "        print(i)\n",
    "        count +=1\n",
    "\n",
    "    #RETORNO DE RESULTADOS\n",
    "    return clusters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MAIN\n",
    "# k = cantidad de clusteres\n",
    "# epochs = cantidad de iteraciones de 'refinación'\n",
    "# redux = cantidad de dimensiones finales que tendrá el dataset (pca puede quejarse si le pones mas de N, siendo N la cantidad de datos)\n",
    "# redux_type 0 = svd, 1 = pca\n",
    "# q = cantidad de veces que se suma y resta la desviación estandar a la media para obtener un rango de generación para las features de los centroides aleatorios. \n",
    "# rango para cada feature = [ft_mean-q*ft_sd, ft_mean+q*ft_sd]\n",
    "\n",
    "# kmeans() retorna un arreglo arr[] en donde cada posición i representa el elemento i del dataset[] ya reducido y procesado, \n",
    "# y arr[i] contiene el número de cluster que se le ha asignado. \n",
    "# El arreglo names[i] contiene el label real del elemento dataset[i]\n",
    "# la reducción de dimensionalidad se hace en la función load(), dejaré un indicador para que sea evidente\n",
    "\n",
    "# k = 7\n",
    "# epochs = 100\n",
    "# redux = 10\n",
    "# redux_type = 0 \n",
    "# q = 2\n",
    "# clusters = kmeans(k, epochs, redux, redux_type, q)\n",
    "\n",
    "#PROBLEMAS ENCONTRADOS:\n",
    "\n",
    "#CLUSTERS CON DOS CATEGORÍAS\n",
    "#Posible respuesta: Algunas categorías de labels no aparecen en un cluster propio, puede deberse a que la naturaleza de los features de dicha categoría sean demasiado similares a otra categoría, por lo que se clusteriza en el mismo sitio o que en la reducción de dimensionalidad, se hayan acortado diferencias entre dos categorías y sus labels, lo que resulta en la primera observación.\n",
    "\n",
    "#MAS DE UN CLUSTER PARA UNA CATEGORÍA\n",
    "#Posible respuesta: Los features de los datos de una misma categoría tienen tendencia a diverger, con la reducción de dimensionalidad, en dos o más grupos diferentes o (la que creo que es) dos centroides aleatorios a1 y a2 cayeron muy cerca entre sí y se formaron dos miniclusters que representan lo mismo. NOTA: como el rango de generación de cada feature para los centroides aleatorios es media+-q*desviacion estandar, si sucede la primera observación, quiere decir que los rangos probablemente resultaron muy pequeños, por lo que se generaron puntos con features similares y habría que aumentar el q para dar mas holgura.\n",
    "\n",
    "#CLUSTERES VACÍOS\n",
    "#Posible respuesta: Se generó un centroide tan lejano que no resultó ser el más cercano para ningún dato, habría que evaluar disminuir el q para disminuir la holgura de generación y crear centroides más cercanos a los puntos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def new_medias(gammas, N, n_samples, n_classes, k_clusters):\n",
    "  medias = np.zeros((k_clusters, n_classes))\n",
    "  for i in range(k_clusters):\n",
    "    media = np.full((n_classes,), 0)\n",
    "    for n in range(n_samples):\n",
    "      media += gammas[n,i]*dataset[n,]\n",
    "    media = media/N[i]\n",
    "    medias[i,] += media\n",
    "  return medias\n",
    "\n",
    "def new_covarianzas(gammas, medias, N, n_samples, n_classes, k_clusters):\n",
    "  covarianzas = np.zeros((k_clusters, n_classes, n_classes))\n",
    "\n",
    "  for i in range(k_clusters):\n",
    "    covarianza = np.zeros((n_classes, n_classes))\n",
    "    for n in range(n_samples):\n",
    "      columna = dataset[n,] - medias[i,]\n",
    "      columna = columna.reshape(n_classes, 1)\n",
    "      fila = columna.reshape(1, n_classes)\n",
    "      covarianza += gammas[n,i]*(columna@fila)\n",
    "    covarianzas[i,:,:] += covarianza/N[i]\n",
    "\n",
    "  return covarianzas\n",
    "\n",
    "def new_N(gammas, k_clusters, n_samples):\n",
    "  N = np.zeros((k_clusters,))\n",
    "  for i in range(k_clusters):\n",
    "    val = 0\n",
    "    for n in range(n_samples):\n",
    "      val += gammas[n,i]\n",
    "    N[i,] += val\n",
    "\n",
    "  return N\n",
    "\n",
    "def new_pis(N, n_samples, k_clusters):\n",
    "  pis = np.zeros((k_clusters,))\n",
    "  for i in range(k_clusters):\n",
    "    pis[i,] += N[i]/n_samples\n",
    "  return pis\n",
    "\n",
    "def new_responsibilities(pis, medias, covarianzas, n_samples, k_clusters):\n",
    "  gammas = np.zeros((n_samples, k_clusters))\n",
    "  for n in range(n_samples):\n",
    "    for k in range(k_clusters):\n",
    "      numerador = pis[k,]*multivariate_normal.pdf(dataset[n,], mean=medias[k,], cov=covarianzas[k,:,:])\n",
    "      denominador = 0\n",
    "      for _k in range(k_clusters):\n",
    "        denominador += pis[_k,]*multivariate_normal.pdf(dataset[n,], mean=medias[_k,], cov=covarianzas[_k,:,:])\n",
    "      res = numerador/denominador\n",
    "      gammas[n,k] += res\n",
    "  return gammas\n",
    "\n",
    "def log_likelihood(k_clusters, medias, covarianzas, n_samples, pis):\n",
    "  prob = 0\n",
    "  for n in range(n_samples):\n",
    "    res = 0\n",
    "    for k in range(k_clusters):\n",
    "      res += pis[k,]*multivariate_normal.pdf(dataset[n,], mean=medias[k,], cov=covarianzas[k,:,:])\n",
    "    res = np.log(res)\n",
    "    prob += res\n",
    "  return prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}